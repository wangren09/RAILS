{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (f1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f5): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from pgd import PGD\n",
    "from tqdm import tqdm\n",
    "from dknn import DKNN\n",
    "\n",
    "ROOT = \"./datasets\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trainset = datasets.CIFAR10(root=ROOT,train=True,transform=transforms.ToTensor())\n",
    "trainloader = DataLoader(trainset,shuffle=True,batch_size=100)\n",
    "\n",
    "testset = datasets.CIFAR10(root=ROOT,train=False,transform=transforms.ToTensor())\n",
    "testloader = DataLoader(testset,shuffle=False,batch_size=50)\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        cfg1 = [64, 64, 'M']\n",
    "        cfg2 = [128, 128, 'M']\n",
    "        cfg3 = [256, 256, 256, 'M']\n",
    "        cfg4 = [512, 512, 512, 'M']\n",
    "        cfg5 = [512, 512, 512, 'M']\n",
    "        self.f1 = self._make_layers(cfg1, 3)\n",
    "        self.f2 = self._make_layers(cfg2, 64)\n",
    "        self.f3 = self._make_layers(cfg3, 128)\n",
    "        self.f4 = self._make_layers(cfg4, 256)\n",
    "        self.f5 = self._make_layers(cfg5, 512)\n",
    "        self.layer = nn.AvgPool2d(kernel_size=1, stride=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.f1(x)\n",
    "        out2 = self.f2(out1)\n",
    "        out3 = self.f3(out2)\n",
    "        out4 = self.f4(out3)\n",
    "        out45 = self.f5(out4)\n",
    "        out5 = self.layer(out45)\n",
    "        out = out5.view(out5.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return [out3, out4, out45, out]\n",
    "    \n",
    "\n",
    "    def _make_layers(self, cfg, in_channels):\n",
    "        layers = []\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "model = VGG()\n",
    "\n",
    "model.load_state_dict(torch.load(\"./model_weights/cifar_vgg16.pt\", map_location=DEVICE)['state_dict'])\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_clfs(\n",
    "    model,\n",
    "    x_train,\n",
    "    hidden_layer=1,\n",
    "    n_neighbors=10,\n",
    "    batch_size=1000,\n",
    "    class_size=1000,\n",
    "    ind=None,\n",
    "    device=DEVICE\n",
    "):\n",
    "    \n",
    "    nn_clfs = []\n",
    "    x_hidden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k,x in enumerate(x_train):\n",
    "            x = x[ind[k]]\n",
    "            xhs = []\n",
    "            for i in range(0,x.size(0),batch_size):\n",
    "                xhs.append(model(x[i:i+batch_size].to(device))[hidden_layer].cpu())\n",
    "            xhs = torch.cat(xhs,dim=0)\n",
    "            x_hidden.append(xhs)\n",
    "            nn_clfs.append(NearestNeighbors(n_neighbors=n_neighbors,n_jobs=-1).fit(xhs.flatten(start_dim=1)))\n",
    "            \n",
    "    return nn_clfs,x_hidden\n",
    "\n",
    "\n",
    "def get_nns(\n",
    "    model,nn_clfs,train_data,train_hidden,x,y,hl=1,input_shape=(3,32,32),device=DEVICE\n",
    "):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hidden = model(x.to(device))[hl].cpu()\n",
    "    n_neighbors = nn_clfs[0].n_neighbors\n",
    "    y_class = [y==i for i in range(10)]\n",
    "    x_class = [x_hidden[yy] for yy in y_class]\n",
    "    nns = []\n",
    "    for i,xx in enumerate(x_class):\n",
    "        if len(xx):\n",
    "            nn_inds = nn_clfs[i].kneighbors(xx.flatten(start_dim=1),return_distance=False)\n",
    "            nns.append(train_data[i][torch.LongTensor(nn_inds)])\n",
    "    nns = torch.cat(nns,dim=0)\n",
    "    nns_reordered = torch.zeros((x.size(0),n_neighbors,)+input_shape)\n",
    "    start_ind = 0\n",
    "    for yy in y_class:\n",
    "        end_ind = start_ind+yy.sum()\n",
    "        nns_reordered[yy] = nns[start_ind:end_ind]\n",
    "        start_ind = end_ind\n",
    "        \n",
    "    return nns_reordered.reshape((-1,)+input_shape),x_hidden\n",
    "\n",
    "\n",
    "def build_neg_clfs(\n",
    "    model,\n",
    "    x_train,\n",
    "    hidden_layer=1,\n",
    "    n_neighbors=1,\n",
    "    batch_size=1000,\n",
    "    class_size=1000,\n",
    "    ind=None,\n",
    "    device=DEVICE\n",
    "):\n",
    "    \n",
    "    nn_clfs = []\n",
    "    x_hidden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k,x in enumerate(x_train):\n",
    "            x = x[ind[k]]\n",
    "            xhs = []\n",
    "            for i in range(0,x.size(0),batch_size):\n",
    "                xhs.append(model(x[i:i+batch_size].to(device))[hidden_layer].cpu())\n",
    "            xhs = torch.cat(xhs,dim=0)\n",
    "            x_hidden.append(xhs)\n",
    "            nn_clfs.append(NearestNeighbors(n_neighbors=n_neighbors,\\\n",
    "                                            n_jobs=-1).fit(xhs.flatten(start_dim=1)))\n",
    "            \n",
    "    return nn_clfs,x_hidden\n",
    "\n",
    "\n",
    "def get_negs(\n",
    "    model,\n",
    "    nn_clfs,\n",
    "    train_data,\n",
    "    train_hidden,\n",
    "    x,\n",
    "    y,\n",
    "    hl=1,\n",
    "    input_shape=(3,32,32),\n",
    "    device=DEVICE,\n",
    "    targeted=False\n",
    "):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hidden = model(x.to(device))[hl].cpu()\n",
    "    n_neighbors = nn_clfs[0].n_neighbors\n",
    "    y_class = [y==i for i in range(10)]\n",
    "    x_class = [x_hidden[yy] for yy in y_class]\n",
    "    nns = []\n",
    "    for i,xx in enumerate(x_class):\n",
    "        if len(xx):\n",
    "            if targeted:\n",
    "                neib_col = []\n",
    "                dist_col = []\n",
    "                for j in range(10):\n",
    "                    if j != i:\n",
    "                        dists,nn_inds = nn_clfs[j].kneighbors(xx.flatten(start_dim=1),return_distance=True)\n",
    "                        neib_col.append(train_data[j][torch.LongTensor(nn_inds)])\n",
    "                        dist_col.append(np.mean(dists,axis=1))\n",
    "                dists = torch.LongTensor(np.stack(dist_col,axis=1))\n",
    "                nns.append(torch.stack(neib_col,dim=1)[range(len(xx)),dists.argmin(dim=-1),:,:,:,:])\n",
    "            else:\n",
    "                neib_col = []\n",
    "                for j in range(10):\n",
    "                    if j != i:\n",
    "                        neib_col.append(train_data[j][torch.LongTensor(\n",
    "                            nn_clfs[j].kneighbors(xx.flatten(start_dim=1),return_distance=False)\n",
    "                            )])\n",
    "                nns.append(torch.cat(neib_col,dim=1))\n",
    "    nns = torch.cat(nns,dim=0)\n",
    "    if targeted:\n",
    "        nns_reordered = torch.zeros((x.size(0),n_neighbors,)+input_shape)                   \n",
    "    else:\n",
    "        nns_reordered = torch.zeros((x.size(0),n_neighbors*9,)+input_shape)\n",
    "    start_ind = 0\n",
    "    for yy in y_class:\n",
    "        end_ind = start_ind+yy.sum()\n",
    "        nns_reordered[yy] = nns[start_ind:end_ind]\n",
    "        start_ind = end_ind\n",
    "    print(nns.shape)\n",
    "    return nns_reordered.reshape((-1,)+input_shape),x_hidden\n",
    "\n",
    "\n",
    "def calc_affinity(nns,x):\n",
    "    \n",
    "    s1, s2, s3, s4, s5 = nns.size()\n",
    "    x = x.unsqueeze(1).repeat_interleave(s2,dim=1)\n",
    "#     x = x.repeat_interleave(s2,dim=0).reshape((s1, s2, s3, s4, s5))\n",
    "    \n",
    "    return (nns-x).pow(2).sum(dim=(1,2,3,4)).sqrt() / s2\n",
    "\n",
    "\n",
    "def KnnAttack(\n",
    "    inp, y_inp, nbd, model, targeted=False, \n",
    "    x_ot=None, rl=True, eps=8/255, step=2/255, \n",
    "    it=10, lamb = 10, layer=1, ch=False, DEVICE=DEVICE\n",
    "):\n",
    "    \n",
    "    choice = ch # set to False if using the softmax-format version\n",
    "    \n",
    "    model.eval()\n",
    "    eta = torch.FloatTensor(*inp.shape).uniform_(-eps, eps)\n",
    "    inp = inp.to(DEVICE)\n",
    "    eta = eta.to(DEVICE)\n",
    "    eta.requires_grad = True\n",
    "    inp.requires_grad = True\n",
    "    tau = 1\n",
    "    shape = model(inp[:2].to(DEVICE))[layer].size()\n",
    "    s1 = shape[1]\n",
    "    s2 = shape[2]\n",
    "    s3 = shape[3]\n",
    "    d1 = inp.size(0)\n",
    "    d2 = nbd.size(0) // d1\n",
    "    \n",
    "    neibor_sc_rep = model(nbd.to(DEVICE))[layer].reshape((d1,d2,s1,s2,s3))\n",
    "    if not rl:\n",
    "        d2_cl = x_ot.size(0) // d1\n",
    "        neibor_dc_rep = model(x_ot.to(DEVICE))[layer].reshape((d1,d2_cl,s1,s2,s3))\n",
    "        # only used for choice False of the untargeted attack\n",
    "        if not choice:\n",
    "            size_cl = d2_cl // 9\n",
    "\n",
    "    for i in range(it):\n",
    "        \n",
    "        inpadv = inp + eta\n",
    "\n",
    "        affinity = calc_affinity(neibor_sc_rep,model(inpadv.to(DEVICE))[layer])\n",
    "        \n",
    "        if rl:\n",
    "            affinity = affinity\n",
    "        else:\n",
    "            affinity = - affinity\n",
    "            if choice or targeted:\n",
    "                negaff = - calc_affinity(neibor_dc_rep,model(inpadv.to(DEVICE))[layer])\n",
    "                affinity = - torch.log(torch.exp(affinity / tau)/(torch.exp(affinity / tau)+torch.exp(negaff / tau)))\n",
    "            # only used for choice 1 of the untargeted attack\n",
    "            else:\n",
    "                negaff = 0\n",
    "                for j in range(9):\n",
    "                    tempaff = - calc_affinity(\n",
    "                        neibor_dc_rep[:,j*size_cl:(j+1)*size_cl],model(inpadv.to(DEVICE))[layer]\n",
    "                    )\n",
    "                    negaff += torch.exp(tempaff / tau)\n",
    "                affinity = - torch.log(torch.exp(affinity / tau)/(torch.exp(affinity / tau)+negaff)).mean()\n",
    "            \n",
    "        affinity = - affinity.mean()\n",
    "        pred_adv = model(inpadv)[-1]\n",
    "        loss = lamb*affinity\n",
    "        grad_sign = torch.autograd.grad(loss, inpadv, only_inputs=True,\\\n",
    "                                        retain_graph = False)[0].sign()\n",
    "        # affinity.backward()\n",
    "        pert = step * grad_sign\n",
    "        inpadv = (inpadv-pert).clamp(0.0,1.0)\n",
    "        tempeta = (inpadv - inp).clamp(-eps, eps)\n",
    "        eta = tempeta\n",
    "        \n",
    "    return inp+eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 9, 3, 32, 32])\n",
      "torch.Size([50, 9, 3, 32, 32])\n",
      "torch.Size([50, 9, 3, 32, 32])\n",
      "torch.Size([50, 9, 3, 32, 32])\n",
      "torch.Size([50, 9, 3, 32, 32])\n",
      "torch.Size([50, 9, 3, 32, 32])\n",
      "torch.Size([50, 9, 3, 32, 32])\n",
      "torch.Size([50, 9, 3, 32, 32])\n",
      "torch.Size([50, 9, 3, 32, 32])\n",
      "{'test_acc': 0.8711111111111111, 'test_acc_rob': 0.3111111111111111, 'test_acc_knn': 0.6488888888888888, 'test_acc_knnrob': 0.34444444444444444, 'test_acc_knnadvdnn': 0.6266666666666667, 'test_acc_knnpgd': 0.5022222222222222}\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(trainset.targets)\n",
    "train_data = [torch.FloatTensor(trainset.data[y_train==i].transpose(0,3,1,2)/255.) for i in range(10)]\n",
    "\n",
    "\n",
    "def test_attack(\n",
    "    y_train,\n",
    "    train_data,\n",
    "    relax=False, # use relaxation or not; In the attack, we will mainly consider relax=False\n",
    "    choice = False, # use which version of contrastive loss, set to False if using the softmax-format version\n",
    "    targeted = False, # whether or not to use targeted attacks\n",
    "    loss_fn = nn.CrossEntropyLoss(),\n",
    "    pgd = PGD(eps=8/255.,step_size=2/255.,max_iter=10),\n",
    "    nn_t_class = 9, # k for the true class\n",
    "    nn_f_class = 9, # k for the other classes\n",
    "    count_up_knn = 10, # max number of batches for evaluation\n",
    "    layers = 0, # the selected layers, choices are 0, 1, 2, 3; 0/1/2 is the third/fourth/fifth layer, 3 is the output\n",
    "    class_samp_size = 1000, #number of samples per class for constructing knn structure\n",
    "    random_state = 1234\n",
    "):\n",
    "\n",
    "    x_neg = None  \n",
    "    nn_clfs = None\n",
    "\n",
    "    test_correct_rob = 0\n",
    "    test_correct = 0\n",
    "    test_correct_knn = 0\n",
    "    test_correct_knnrob = 0\n",
    "    test_correct_knnadvdnn = 0\n",
    "    test_correct_knnpgd = 0\n",
    "    test_total = 0\n",
    "    test_total_knn = 0\n",
    "    count_knn_eval = 0\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    ind_samp =  [np.random.choice(np.arange(5000),size=class_samp_size,replace=False) for i in range(10)]\n",
    "    ind_samp_data = [np.where(y_train==i)[0][ind_samp[i]] for i in range(10)]\n",
    "    ind_samp_flat = [x for sublist in ind_samp_data for x in sublist]\n",
    "    train_samp = torch.FloatTensor(trainset.data.transpose(0,3,1,2)/255.)[ind_samp_flat]\n",
    "    y_samp = torch.LongTensor(y_train)[ind_samp_flat]\n",
    "\n",
    "    model.eval()\n",
    "    dknn = DKNN(model, train_samp, y_samp, hidden_layers=[layers+2], device=DEVICE)\n",
    "\n",
    "\n",
    "    train_data_samp = [torch.FloatTensor(trainset.data[ind_samp_data[i]].transpose(0,3,1,2)/255.) for i in range(10)]\n",
    "\n",
    "    nn_clfs, train_hidden = build_nn_clfs(\n",
    "        model,train_data,hidden_layer=layers,n_neighbors=nn_t_class,class_size=class_samp_size,ind=ind_samp\n",
    "    )\n",
    "\n",
    "    if not relax:\n",
    "        neg_clfs, neg_hidden = build_neg_clfs(\n",
    "            model,train_data,hidden_layer=layers,n_neighbors=nn_f_class,class_size=class_samp_size,ind=ind_samp\n",
    "        )\n",
    "\n",
    "    for x,y in testloader:\n",
    "        count_knn_eval = count_knn_eval + 1\n",
    "\n",
    "        if count_knn_eval < count_up_knn:\n",
    "            x_adv = pgd.generate(model,x,y,device=DEVICE)\n",
    "            x_mem, _ = get_nns(model,nn_clfs,train_data_samp,train_hidden,x,y,hl=layers)\n",
    "            if not relax:\n",
    "                x_neg, _ = get_negs(model,neg_clfs,train_data_samp,neg_hidden,x,y,hl=layers,targeted=targeted)\n",
    "            x_knnadv = KnnAttack(\n",
    "                x, y, x_mem, model, targeted=targeted, \n",
    "                x_ot = x_neg, rl = relax, \n",
    "                eps=8/255, step=2/255, it=20, lamb = 1000, layer=layers, ch=choice, DEVICE=DEVICE\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if count_knn_eval < count_up_knn:\n",
    "                pred = model(x.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                test_correct += (pred==y.to(DEVICE)).sum().item()\n",
    "                pred_adv = model(x_adv.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                test_correct_rob += (pred_adv==y.to(DEVICE)).sum().item()\n",
    "                test_total += x.size(0)\n",
    "\n",
    "                # knn attack acc\n",
    "                pred_dknn = dknn(x.to(DEVICE)).argmax(axis=1)\n",
    "                pred_knnadv = dknn(x_knnadv.to(DEVICE)).argmax(axis=1)\n",
    "                pred_knnadvdnn = model(x_knnadv.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                pred_knnpgdadv = dknn(x_adv.to(DEVICE)).argmax(axis=1)\n",
    "                test_correct_knn += (pred_dknn==y.numpy()).astype(\"float\").sum()\n",
    "                test_correct_knnadvdnn += (pred_knnadvdnn==y.to(DEVICE)).sum().item()\n",
    "                test_correct_knnrob += (pred_knnadv==y.numpy()).astype(\"float\").sum()\n",
    "                test_correct_knnpgd += (pred_knnpgdadv==y.numpy()).astype(\"float\").sum()\n",
    "                test_total_knn += x.size(0)\n",
    "\n",
    "    print({\n",
    "        \"test_acc\": test_correct/test_total,\n",
    "        \"test_acc_rob\": test_correct_rob/test_total,\n",
    "        \"test_acc_knn\": test_correct_knn/test_total_knn,\n",
    "        \"test_acc_knnrob\": test_correct_knnrob/test_total_knn,\n",
    "        \"test_acc_knnadvdnn\": test_correct_knnadvdnn/test_total_knn,\n",
    "        \"test_acc_knnpgd\": test_correct_knnpgd/test_total_knn\n",
    "    })\n",
    "    \n",
    "test_attack(\n",
    "    y_train,\n",
    "    train_data,\n",
    "    targeted = True, # whether or not to use targeted attacks\n",
    "    random_state = 135\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "y_train = np.array(trainset.targets)\n",
    "train_data = [torch.FloatTensor(trainset.data[y_train==i].transpose(0,3,1,2)/255.) for i in range(10)]\n",
    "\n",
    "# use targeted attack\n",
    "targeted = False\n",
    "\n",
    "# use relaxation or not; In the attack, we will mainly consider relax=False\n",
    "relax = False\n",
    "x_neg = None\n",
    "\n",
    "# use which version of contrastive loss, set to False if using the softmax-format version\n",
    "choice = False\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "pgd = PGD(eps=8/255.,step_size=2/255.,max_iter=10)\n",
    "nn_clfs = None\n",
    "layers = 0 # the selected layers, choices are 0, 1, 2, 3; 0/1/2 is the third/fourth/fifth layer, 3 is the output\n",
    "class_samp_size = 1000 #number of samples per class for constructing knn structure\n",
    "nn_t_class = 9 # k for the true class\n",
    "nn_f_class = 9 # k for the other classes\n",
    "  \n",
    "count_up_knn = 10\n",
    "test_correct_rob = 0\n",
    "test_correct = 0\n",
    "test_correct_knn = 0\n",
    "test_correct_knnrob = 0\n",
    "test_correct_knnpgd = 0\n",
    "test_total = 0\n",
    "test_total_knn = 0\n",
    "count_knn_eval = 0\n",
    "\n",
    "np.random.seed(1234)\n",
    "ind_samp =  [np.random.choice(np.arange(5000),size=class_samp_size,replace=False) for i in range(10)]\n",
    "ind_samp_data = [np.where(y_train==i)[0][ind_samp[i]] for i in range(10)]\n",
    "ind_samp_flat = [x for sublist in ind_samp_data for x in sublist]\n",
    "train_samp = torch.FloatTensor(trainset.data.transpose(0,3,1,2)/255.)[ind_samp_flat]\n",
    "y_samp = torch.LongTensor(y_train)[ind_samp_flat]\n",
    "\n",
    "model.eval()\n",
    "dknn = DKNN(model, train_samp, y_samp, hidden_layers=[layers+2], device=DEVICE)\n",
    "\n",
    "\n",
    "train_data_samp = [torch.FloatTensor(trainset.data[ind_samp_data[i]].transpose(0,3,1,2)/255.) for i in range(10)]\n",
    "\n",
    "nn_clfs, train_hidden = build_nn_clfs(\n",
    "    model,train_data,hidden_layer=layers,n_neighbors=nn_t_class,class_size=class_samp_size,ind=ind_samp\n",
    ")\n",
    "\n",
    "if not relax:\n",
    "    neg_clfs, neg_hidden = build_neg_clfs(\n",
    "        model,train_data,hidden_layer=layers,n_neighbors=nn_f_class,class_size=class_samp_size,ind=ind_samp\n",
    "    )\n",
    "\n",
    "for x,y in testloader:\n",
    "    count_knn_eval = count_knn_eval + 1\n",
    "    \n",
    "    if count_knn_eval < count_up_knn:\n",
    "        x_adv = pgd.generate(model,x,y,device=DEVICE)\n",
    "        x_mem, _ = get_nns(model,nn_clfs,train_data_samp,train_hidden,x,y,hl=layers)\n",
    "        if not relax:\n",
    "            x_neg, _ = get_negs(model,neg_clfs,train_data_samp,neg_hidden,x,y,hl=layers,targeted=targeted)\n",
    "        x_knnadv = KnnAttack(\n",
    "            x, y, x_mem, model, targeted=targeted, \n",
    "            x_ot = x_neg, rl = relax, \n",
    "            eps=8/255, step=2/255, it=20, lamb = 1000, layer=layers, ch=choice, DEVICE=DEVICE\n",
    "        )\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        if count_knn_eval < count_up_knn:\n",
    "            pred = model(x.to(DEVICE))[-1].max(dim=1)[1]\n",
    "            test_correct += (pred==y.to(DEVICE)).sum().item()\n",
    "            pred_adv = model(x_adv.to(DEVICE))[-1].max(dim=1)[1]\n",
    "            test_correct_rob += (pred_adv==y.to(DEVICE)).sum().item()\n",
    "            test_total += x.size(0)\n",
    "        \n",
    "            # knn attack acc\n",
    "            pred_dknn = dknn(x.to(DEVICE)).argmax(axis=1)\n",
    "            pred_knnadv = dknn(x_knnadv.to(DEVICE)).argmax(axis=1)\n",
    "            pred_knnpgdadv = dknn(x_adv.to(DEVICE)).argmax(axis=1)\n",
    "            test_correct_knn += (pred_dknn==y.numpy()).astype(\"float\").sum()\n",
    "            test_correct_knnrob += (pred_knnadv==y.numpy()).astype(\"float\").sum()\n",
    "            test_correct_knnpgd += (pred_knnpgdadv==y.numpy()).astype(\"float\").sum()\n",
    "            test_total_knn += x.size(0)\n",
    "            \n",
    "print({\n",
    "    \"test_acc\": test_correct/test_total,\n",
    "    \"test_acc_rob\": test_correct_rob/test_total,\n",
    "    \"test_acc_knn\": test_correct_knn/test_total_knn,\n",
    "    \"test_acc_knnrob\": test_correct_knnrob/test_total_knn,\n",
    "    \"test_acc_knnpgd\": test_correct_knnpgd/test_total_knn\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
