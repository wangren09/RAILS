{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "#from models.VGG import VGG\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from pgd import PGD\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT = \"./datasets\"\n",
    "\n",
    "trainset = datasets.CIFAR10(root=ROOT,train=True,transform=transforms.ToTensor())\n",
    "trainloader = DataLoader(trainset,shuffle=True,batch_size=128)\n",
    "\n",
    "testset = datasets.CIFAR10(root=ROOT,train=False,transform=transforms.ToTensor())\n",
    "testloader = DataLoader(testset,shuffle=True,batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = VGG()\n",
    "\n",
    "# class Extractor(nn.Module):\n",
    "#     def __init__(self,model,hidden_layer):\n",
    "#         super(Extractor,self).__init__()\n",
    "#         self.hidden_layer = hidden_layer\n",
    "#         if len(self.hidden_layer) == 1:\n",
    "#             self.feature,self.out = self._get_layer(model)\n",
    "#         if len(self.hidden_layer) == 2:\n",
    "#             self.feature1, self.feature, self.out = self._get_layer(model)\n",
    "#         if len(self.hidden_layer) == 3:\n",
    "#             self.feature1, self.feature2, self.feature, self.out = self._get_layer(model)\n",
    "#     def _get_layer(self,model):\n",
    "#         children = list(model.named_children())\n",
    "#         for i,(name,mod) in enumerate(children):\n",
    "#             if \"classifier\" in name:\n",
    "#                 break\n",
    "#         children.insert(i,(\"flatten\",nn.Flatten(start_dim=1)))\n",
    "#         if len(self.hidden_layer) == 1:\n",
    "#             return nn.Sequential(OrderedDict(children[:self.hidden_layer[0]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[0]+1:]))\n",
    "#         elif len(self.hidden_layer) == 2:\n",
    "#             return nn.Sequential(OrderedDict(children[:self.hidden_layer[0]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[0]+1:self.hidden_layer[1]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[1]+1:]))\n",
    "#         else:\n",
    "#             return nn.Sequential(OrderedDict(children[:self.hidden_layer[0]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[0]+1:self.hidden_layer[1]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[1]+1:self.hidden_layer[2]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[2]+1:]))\n",
    "#     def forward(self,x):\n",
    "#         if len(self.hidden_layer) == 3:\n",
    "#             feature1 = self.feature1(x)\n",
    "#             feature2 = self.feature2(feature1)\n",
    "#             feature = self.feature(feature2)\n",
    "#             out = self.out(feature)\n",
    "#             return [feature1,feature2,feature,out]\n",
    "#         if len(self.hidden_layer) == 2:\n",
    "#             feature1 = self.feature1(x)\n",
    "#             feature = self.feature(feature1)\n",
    "#             out = self.out(feature)\n",
    "#             return [feature1,feature,out]\n",
    "#         else:\n",
    "#             feature = self.feature(x)\n",
    "#             out = self.out(feature)\n",
    "#             return [feature,out]\n",
    "    \n",
    "# model = Extractor(model,[5])\n",
    "# model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (f1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f5): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = VGG()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        #self.features = self._make_layers(cfg[vgg_name])\n",
    "        cfg1 = [64, 64, 'M']\n",
    "        cfg2 = [128, 128, 'M']\n",
    "        cfg3 = [256, 256, 256, 'M']\n",
    "        cfg4 = [512, 512, 512, 'M']\n",
    "        cfg5 = [512, 512, 512, 'M']\n",
    "        self.f1 = self._make_layers(cfg1, 3)\n",
    "        self.f2 = self._make_layers(cfg2, 64)\n",
    "        self.f3 = self._make_layers(cfg3, 128)\n",
    "        self.f4 = self._make_layers(cfg4, 256)\n",
    "        self.f5 = self._make_layers(cfg5, 512)\n",
    "        self.layer = nn.AvgPool2d(kernel_size=1, stride=1)\n",
    "        #self.classifier = nn.Linear(512, 10)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.f1(x)\n",
    "        out2 = self.f2(out1)\n",
    "        out3 = self.f3(out2)\n",
    "        out4 = self.f4(out3)\n",
    "        out45 = self.f5(out4)\n",
    "        out5 = self.layer(out45)\n",
    "        out = out5.view(out5.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return [out4, out45, out]\n",
    "    \n",
    "\n",
    "    def _make_layers(self, cfg, in_channels):\n",
    "        layers = []\n",
    "#         in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "model = VGG()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_clfs(model,x_train,hidden_layer=3,n_neighbors=10,\\\n",
    "                  batch_size=1000,class_size=1000,device=DEVICE):\n",
    "    nn_clfs = []\n",
    "    x_hidden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k,x in enumerate(x_train):\n",
    "            x = x[np.random.choice(np.arange(x.size(0)),size=class_size,replace=False)]\n",
    "            xhs = []\n",
    "            for i in range(0,x.size(0),batch_size):\n",
    "                xhs.append(model(x[i:i+batch_size].to(device))[1].cpu())\n",
    "            xhs = torch.cat(xhs,dim=0)\n",
    "            x_hidden.append(xhs)\n",
    "            nn_clfs.append(NearestNeighbors(n_neighbors=n_neighbors,\\\n",
    "                                            n_jobs=-1).fit(xhs.flatten(start_dim=1)))\n",
    "    return nn_clfs,x_hidden\n",
    "\n",
    "\n",
    "def get_nns(model,nn_clfs,train_data,train_hidden,x,y,hl=3,\\\n",
    "            input_shape=(3,32,32),device=DEVICE):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hidden = model(x.to(device))[1].cpu()\n",
    "    n_neighbors = nn_clfs[0].n_neighbors\n",
    "    y_class = [y==i for i in range(10)]\n",
    "    x_class = [x_hidden[yy] for yy in y_class]\n",
    "    nns = []\n",
    "    for i,xx in enumerate(x_class):\n",
    "        nn_inds = nn_clfs[i].kneighbors(xx.flatten(start_dim=1),return_distance=False)\n",
    "        nns.append(train_data[i][torch.LongTensor(nn_inds)])\n",
    "    nns = torch.cat(nns,dim=0)\n",
    "    nns_reordered = torch.zeros((x.size(0),n_neighbors,)+input_shape)\n",
    "    start_ind = 0\n",
    "    for yy in y_class:\n",
    "        end_ind = start_ind+yy.sum()\n",
    "        nns_reordered[yy] = nns[start_ind:end_ind]\n",
    "        start_ind = end_ind\n",
    "    return nns_reordered.reshape((-1,)+input_shape),x_hidden\n",
    "\n",
    "def calc_affinity(nns,x):\n",
    "    return (nns-x.repeat_interleave(nns.size(0)//x.size(0),dim=0)\\\n",
    "           ).pow(2).sum(dim=(1,2,3)).sqrt().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neg_clfs(model,x_train,hidden_layer=3,n_neighbors=1,\\\n",
    "                  batch_size=1000,class_size=1000,device=DEVICE):\n",
    "    nn_clfs = []\n",
    "    x_hidden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k,x in enumerate(x_train):\n",
    "            x = x[np.random.choice(np.arange(x.size(0)),size=class_size,replace=False)]\n",
    "            xhs = []\n",
    "            for i in range(0,x.size(0),batch_size):\n",
    "                xhs.append(model(x[i:i+batch_size].to(device))[1].cpu())\n",
    "            xhs = torch.cat(xhs,dim=0)\n",
    "            x_hidden.append(xhs)\n",
    "            nn_clfs.append(NearestNeighbors(n_neighbors=n_neighbors,\\\n",
    "                                            n_jobs=-1).fit(xhs.flatten(start_dim=1)))\n",
    "    return nn_clfs,x_hidden\n",
    "\n",
    "\n",
    "def get_negs(model,nn_clfs,train_data,train_hidden,x,y,hl=3,\\\n",
    "            input_shape=(3,32,32),device=DEVICE):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hidden = model(x.to(device))[1].cpu()\n",
    "    n_neighbors = nn_clfs[0].n_neighbors*9\n",
    "    y_class = [y==i for i in range(10)]\n",
    "    x_class = [x_hidden[yy] for yy in y_class]\n",
    "    nns = []\n",
    "    for i,xx in enumerate(x_class):\n",
    "        for j in range(10):\n",
    "            if j != i:\n",
    "                nn_inds = nn_clfs[j].kneighbors(xx.flatten(start_dim=1),\\\n",
    "                                                return_distance=False)\n",
    "                if (i == 0 and j == 1) or (i > 0 and j == 0):\n",
    "                    neib_col = train_data[j][torch.LongTensor(nn_inds)]\n",
    "                else:\n",
    "                    neib_col = torch.cat((neib_col,\\\n",
    "                                          train_data[j][torch.LongTensor(nn_inds)]),1)\n",
    "        nns.append(neib_col)\n",
    "    nns = torch.cat(nns,dim=0)\n",
    "    nns_reordered = torch.zeros((x.size(0),n_neighbors,)+input_shape)\n",
    "    start_ind = 0\n",
    "    for yy in y_class:\n",
    "        end_ind = start_ind+yy.sum()\n",
    "        nns_reordered[yy] = nns[start_ind:end_ind]\n",
    "        start_ind = end_ind\n",
    "    return nns_reordered.reshape((-1,)+input_shape),x_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KnnAttack(inp, y_inp, nbd, model, x_ot=None, rl=True,\\\n",
    "              eps=4/255, step=2/255, it=10, lamb = 10, DEVICE=DEVICE):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    eta = torch.FloatTensor(*inp.shape).uniform_(-eps, eps)\n",
    "    inp = inp.to(DEVICE)\n",
    "    eta = eta.to(DEVICE)\n",
    "    eta.requires_grad = True\n",
    "    inp.requires_grad = True\n",
    "    #feature = model.feature(x_adv.to(DEVICE))\n",
    "    for i in range(it):\n",
    "        inpadv = inp + eta\n",
    "\n",
    "        affinity = calc_affinity(model(nbd.to(DEVICE))[1],\\\n",
    "                                 model(inpadv.to(DEVICE))[1]) / 9\n",
    "        \n",
    "        if rl:\n",
    "            affinity = affinity\n",
    "        else:\n",
    "            negaff = calc_affinity(model(x_ot.to(DEVICE))[1],\\\n",
    "                            model(inpadv.to(DEVICE))[1]) / 9\n",
    "            affinity = torch.log(torch.exp(affinity) / (torch.exp(affinity)+torch.exp(negaff)))\n",
    "        affinity = - affinity\n",
    "        pred_adv = model(inpadv)[-1]\n",
    "        loss_ce = - loss_fn(pred_adv, y_inp.to(DEVICE))\n",
    "        loss = loss_ce + lamb*affinity\n",
    "        grad_sign = torch.autograd.grad(loss, inpadv, only_inputs=True,\\\n",
    "                                        retain_graph = False)[0].sign()\n",
    "        #affinity.backward()\n",
    "        pert = step * grad_sign\n",
    "        inpadv = (inpadv-pert).clamp(0.0,1.0)\n",
    "        tempeta = (inpadv - inp).clamp(-eps, eps)\n",
    "        eta = tempeta\n",
    "    return inp+eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(trainset.targets)\n",
    "train_data = [torch.FloatTensor(trainset.data[y_train==i].transpose(0,3,1,2)/255.) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/30 epochs::  99%|█████████▊| 386/391 [00:19<00:00, 89.08it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\u001b[A\n",
      "1/30 epochs::  99%|█████████▊| 386/391 [00:48<00:00,  7.91it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (999) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:47",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-71a443d9a9e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                     \u001b[0mx_knnadv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdknnatt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RAILS_PaperSub/dknn_attack.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, x, y, targets)\u001b[0m\n\u001b[1;32m    222\u001b[0m                     pert, targets, nn_labels, weights, nn_reprs = self.attack(\n\u001b[1;32m    223\u001b[0m                         \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                         \u001b[0my_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                     )\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RAILS_PaperSub/dknn_attack.py\u001b[0m in \u001b[0;36mattack\u001b[0;34m(self, x, y, pert, targets, nn_labels, weights, nn_reprs, eps, lr, random_init)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mstart_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_neighborhood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mend_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_neighborhood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_repr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_repr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_ind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"grad\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mpert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RAILS_PaperSub/dknn_attack.py\u001b[0m in \u001b[0;36mknn_loss\u001b[0;34m(self, l, hidden_repr, nn_repr, n_samples, weights, alpha)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mknn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_repr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_repr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         l2dist = (\n\u001b[1;32m    146\u001b[0m                 \u001b[0mhidden_repr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnn_repr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_neighborhood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m             raise AssertionError(\n\u001b[1;32m    189\u001b[0m                 \"libcudart functions unavailable. It looks like you have a broken build?\")\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (999) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:47"
     ]
    }
   ],
   "source": [
    "from dknn import DKNN\n",
    "from dknn_attack import DKNNAttack\n",
    "\n",
    "BURN_IN = 0\n",
    "EPS = 0.02\n",
    "#use relaxation or not\n",
    "relax = True\n",
    "x_neg = None\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(),lr=1e-3,momentum=0.9,weight_decay=1e-4,nesterov=True)\n",
    "pgd = PGD(eps=8/255.,step=2/255.,max_iter=10)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer,step_size=50,gamma=0.1)\n",
    "EPOCHS = 30\n",
    "nn_clfs = None\n",
    "lt1 = 1#0.01  #penalty on knn loss\n",
    "lt2 = 100#100\n",
    "layers = 3\n",
    "\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "    \n",
    "    \n",
    "    #if ep>=BURN_IN and not (ep-BURN_IN)%1:\n",
    "    if ep >= 0:\n",
    "        nn_clfs, train_hidden = build_nn_clfs(model,train_data,hidden_layer=layers,\\\n",
    "                                              n_neighbors=9)\n",
    "    if not relax:\n",
    "        neg_clfs, neg_hidden = build_neg_clfs(model,train_data,hidden_layer=layers,\\\n",
    "                                              n_neighbors=1)\n",
    "    \n",
    "    train_loss = 0.\n",
    "    train_correct = 0.\n",
    "    train_total = 0.\n",
    "\n",
    "    with tqdm(trainloader,desc=f\"{ep+1}/{EPOCHS} epochs:\") as t:\n",
    "        for i,(x,y) in enumerate(t):\n",
    "            model.train()\n",
    "#             if nn_clfs is not None:\n",
    "#                 relax = True\n",
    "#                 x_mem, _ = get_nns(model,nn_clfs,train_data,train_hidden,x,y)\n",
    "#                 if not relax:\n",
    "#                     x_neg, _ = get_negs(model,neg_clfs,train_data,neg_hidden,x,y)\n",
    "#                 x_adv = KnnAttack(x, y, x_mem, model, x_ot = x_neg, rl = relax, eps=4/255,\\\n",
    "#                                   step=2/255,it=10, lamb = lt2, DEVICE=DEVICE)\n",
    "#                 model.train()\n",
    "#                 *_,out = model(x_adv.detach().to(DEVICE))\n",
    "#                 loss_ce = loss_fn(out,y.to(DEVICE))\n",
    "#                 aff = calc_affinity(model(x_mem.to(DEVICE))[1],\\\n",
    "#                                     model(x_adv.to(DEVICE))[1]) / 9\n",
    "#                 if relax:\n",
    "#                     aff = aff\n",
    "#                 else:\n",
    "#                     negaff = calc_affinity(model(x_neg.to(DEVICE))[1],\\\n",
    "#                                     model(x_adv.to(DEVICE))[1]) / 9\n",
    "#                     aff = torch.log(torch.exp(aff) / (torch.exp(aff)+torch.exp(negaff)))\n",
    "#                 loss = loss_ce + lt1*aff\n",
    "#                 train_loss += loss.item()\n",
    "#                 pred = out.max(1)[1].detach().cpu()\n",
    "#                 train_correct += (pred==y).sum().item()\n",
    "#                 train_total += x.size(0)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 t.set_postfix({\n",
    "#                     \"train_loss\": train_loss/train_total,\n",
    "#                     \"train_acc\": train_correct/train_total\n",
    "#                 })\n",
    "#             else:\n",
    "#                 model.train()\n",
    "#                 *_,out = model(x.to(DEVICE))\n",
    "#                 loss = loss_fn(out,y.to(DEVICE))\n",
    "#                 train_loss += loss.item()*x.size(0)\n",
    "#                 pred = out.max(dim=1)[1].detach().cpu()\n",
    "#                 train_correct += (pred==y).sum().item()\n",
    "#                 train_total += x.size(0)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 t.set_postfix({\n",
    "#                     \"train_loss\": train_loss/train_total,\n",
    "#                     \"train_acc\": train_correct/train_total\n",
    "#                 })\n",
    "            if i == len(trainloader)-1:\n",
    "                test_correct_rob = 0\n",
    "                test_correct = 0\n",
    "                test_correct_knn = 0\n",
    "                test_correct_knnrob = 0\n",
    "                test_total = 0\n",
    "                test_total_knn = 0\n",
    "                count_knn_eval = 0\n",
    "                \n",
    "                index = np.random.choice(np.arange(50000),\\\n",
    "                         size=4000,replace=False)\n",
    "                train_samp = torch.FloatTensor(trainset.data.\\\n",
    "                                               transpose(0,3,1,2)/255.)[index]\n",
    "                y_samp = torch.LongTensor(y_train)[index]\n",
    "                model.eval()\n",
    "                dknn = DKNN(model, train_samp, y_samp,\\\n",
    "                            hidden_layers=[4], device=DEVICE)\n",
    "                    \n",
    "                dknnatt = DKNNAttack(\n",
    "                    model,\n",
    "                    train_samp,\n",
    "                    y_samp,\n",
    "                    hidden_layers=[4],\n",
    "                device=DEVICE)\n",
    "                for x,y in testloader:\n",
    "                    count_knn_eval = count_knn_eval + 1\n",
    "                    x_adv = pgd.generate(model,x,y,device=DEVICE)\n",
    "                    #knn attack (informal)\n",
    "#                     nn_clfs, train_hidden = build_nn_clfs(model,train_data,\\\n",
    "#                                                           hidden_layer=layers,n_neighbors=9)\n",
    "                    \n",
    "#                     if not relax:\n",
    "#                         neg_clfs, neg_hidden = build_neg_clfs(model,train_data,\\\n",
    "#                                                           hidden_layer=layers,n_neighbors=1)\n",
    "                    \n",
    "#                     x_mem, _ = get_nns(model,nn_clfs,train_data,train_hidden,x,y)\n",
    "#                     if not relax:\n",
    "#                         x_neg, _ = get_negs(model,neg_clfs,train_data,neg_hidden,x,y)\n",
    "#                     x_knnadv = KnnAttack(x, y, x_mem, model, x_ot = x_neg, rl = relax, eps=4/255,\\\n",
    "#                                   step=2/255,it=10, lamb = 10000, DEVICE=DEVICE)\n",
    "                    \n",
    "                    \n",
    "                    x_knnadv = dknnatt.generate(x, y)\n",
    "    \n",
    "    \n",
    "    ####\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(x.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                        test_correct += (pred==y.to(DEVICE)).sum().item()\n",
    "                        pred_adv = model(x_adv.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                        test_correct_rob += (pred_adv==y.to(DEVICE)).sum().item()\n",
    "                        test_total += x.size(0)\n",
    "                        \n",
    "                        if count_knn_eval < 10:\n",
    "                        #knn attack acc\n",
    "                            pred_dknn = dknn(x.to(DEVICE)).argmax(axis=1)\n",
    "                            pred_knnadv = dknn(x_knnadv.to(DEVICE)).argmax(axis=1)\n",
    "                        #pred_knnadv = model(x_knnadv.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                            test_correct_knn += (pred_dknn==y.to(DEVICE)).sum().item()\n",
    "                            test_correct_knnrob += (pred_knnadv==y.to(DEVICE)).sum().item()\n",
    "                            test_total_knn += x.size(0)\n",
    "                        #\n",
    "                t.set_postfix({\n",
    "#                     \"train_loss\": train_loss/train_total,\n",
    "#                     \"train_acc\": train_correct/train_total,\n",
    "                    \"test_acc\": test_correct/test_total,\n",
    "                    \"test_acc_rob\": test_correct_rob/test_total,\n",
    "                    \"test_acc_knn\": test_correct_knn/test_total_knn,\n",
    "                    \"test_acc_knnrob\": test_correct_knnrob/test_total_knn\n",
    "                })\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
