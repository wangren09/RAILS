{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from models.VGG import VGG\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from pgd import PGD\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT = \"./datasets\"\n",
    "\n",
    "trainset = datasets.CIFAR10(root=ROOT,train=True,transform=transforms.ToTensor())\n",
    "trainloader = DataLoader(trainset,shuffle=True,batch_size=128)\n",
    "\n",
    "testset = datasets.CIFAR10(root=ROOT,train=False,transform=transforms.ToTensor())\n",
    "testloader = DataLoader(testset,shuffle=True,batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extractor(\n",
       "  (feature): Sequential(\n",
       "    (f1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (f2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (f3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (f4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (f5): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (flatten): Flatten()\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "      (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Linear(in_features=512, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VGG()\n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self,model,hidden_layer):\n",
    "        super(Extractor,self).__init__()\n",
    "        self.hidden_layer = hidden_layer\n",
    "        if len(self.hidden_layer) == 1:\n",
    "            self.feature,self.out = self._get_layer(model)\n",
    "        if len(self.hidden_layer) == 2:\n",
    "            self.feature1, self.feature, self.out = self._get_layer(model)\n",
    "        if len(self.hidden_layer) == 3:\n",
    "            self.feature1, self.feature2, self.feature, self.out = self._get_layer(model)\n",
    "    def _get_layer(self,model):\n",
    "        children = list(model.named_children())\n",
    "        for i,(name,mod) in enumerate(children):\n",
    "            if \"classifier\" in name:\n",
    "                break\n",
    "        children.insert(i,(\"flatten\",nn.Flatten(start_dim=1)))\n",
    "        if len(self.hidden_layer) == 1:\n",
    "            return nn.Sequential(OrderedDict(children[:self.hidden_layer[0]+1])), \\\n",
    "            nn.Sequential(OrderedDict(children[self.hidden_layer[0]+1:]))\n",
    "        elif len(self.hidden_layer) == 2:\n",
    "            return nn.Sequential(OrderedDict(children[:self.hidden_layer[0]+1])), \\\n",
    "            nn.Sequential(OrderedDict(children[self.hidden_layer[0]+1:self.hidden_layer[1]+1])), \\\n",
    "            nn.Sequential(OrderedDict(children[self.hidden_layer[1]+1:]))\n",
    "        else:\n",
    "            return nn.Sequential(OrderedDict(children[:self.hidden_layer[0]+1])), \\\n",
    "            nn.Sequential(OrderedDict(children[self.hidden_layer[0]+1:self.hidden_layer[1]+1])), \\\n",
    "            nn.Sequential(OrderedDict(children[self.hidden_layer[1]+1:self.hidden_layer[2]+1])), \\\n",
    "            nn.Sequential(OrderedDict(children[self.hidden_layer[2]+1:]))\n",
    "    def forward(self,x):\n",
    "        if len(self.hidden_layer) == 3:\n",
    "            feature1 = self.feature1(x)\n",
    "            feature2 = self.feature2(feature1)\n",
    "            feature = self.feature(feature2)\n",
    "            out = self.out(feature)\n",
    "            return [feature1,feature2,feature,out]\n",
    "        if len(self.hidden_layer) == 2:\n",
    "            feature1 = self.feature1(x)\n",
    "            feature = self.feature(feature1)\n",
    "            out = self.out(feature)\n",
    "            return [feature1,feature,out]\n",
    "        else:\n",
    "            feature = self.feature(x)\n",
    "            out = self.out(feature)\n",
    "            return [feature,out]\n",
    "    \n",
    "model = Extractor(model,[5])\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_clfs(model,x_train,hidden_layer=3,n_neighbors=10,\\\n",
    "                  batch_size=1000,class_size=2000,device=DEVICE):\n",
    "    nn_clfs = []\n",
    "    x_hidden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k,x in enumerate(x_train):\n",
    "            x = x[np.random.choice(np.arange(x.size(0)),size=class_size,replace=False)]\n",
    "            xhs = []\n",
    "            for i in range(0,x.size(0),batch_size):\n",
    "                xhs.append(model.feature(x[i:i+batch_size].to(device)).cpu())\n",
    "            xhs = torch.cat(xhs,dim=0)\n",
    "            x_hidden.append(xhs)\n",
    "            nn_clfs.append(NearestNeighbors(n_neighbors=n_neighbors,\\\n",
    "                                            n_jobs=-1).fit(xhs.flatten(start_dim=1)))\n",
    "    return nn_clfs,x_hidden\n",
    "\n",
    "\n",
    "def get_nns(model,nn_clfs,train_data,train_hidden,x,y,hl=3,\\\n",
    "            input_shape=(3,32,32),device=DEVICE):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hidden = model.feature(x.to(device)).cpu()\n",
    "    n_neighbors = nn_clfs[0].n_neighbors\n",
    "    y_class = [y==i for i in range(10)]\n",
    "    x_class = [x_hidden[yy] for yy in y_class]\n",
    "    nns = []\n",
    "    for i,xx in enumerate(x_class):\n",
    "        nn_inds = nn_clfs[i].kneighbors(xx.flatten(start_dim=1),return_distance=False)\n",
    "        nns.append(train_data[i][torch.LongTensor(nn_inds)])\n",
    "    nns = torch.cat(nns,dim=0)\n",
    "    nns_reordered = torch.zeros((x.size(0),n_neighbors,)+input_shape)\n",
    "    start_ind = 0\n",
    "    for yy in y_class:\n",
    "        end_ind = start_ind+yy.sum()\n",
    "        nns_reordered[yy] = nns[start_ind:end_ind]\n",
    "        start_ind = end_ind\n",
    "    return nns_reordered.reshape((-1,)+input_shape),x_hidden\n",
    "\n",
    "def calc_affinity(nns,x):\n",
    "    return (nns-x.repeat_interleave(nns.size(0)//x.size(0),dim=0)\\\n",
    "           ).pow(2).sum(dim=(1,2,3)).sqrt().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neg_clfs(model,x_train,hidden_layer=3,n_neighbors=1,\\\n",
    "                  batch_size=1000,class_size=2000,device=DEVICE):\n",
    "    nn_clfs = []\n",
    "    x_hidden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k,x in enumerate(x_train):\n",
    "            x = x[np.random.choice(np.arange(x.size(0)),size=class_size,replace=False)]\n",
    "            xhs = []\n",
    "            for i in range(0,x.size(0),batch_size):\n",
    "                xhs.append(model.feature(x[i:i+batch_size].to(device)).cpu())\n",
    "            xhs = torch.cat(xhs,dim=0)\n",
    "            x_hidden.append(xhs)\n",
    "            nn_clfs.append(NearestNeighbors(n_neighbors=n_neighbors,\\\n",
    "                                            n_jobs=-1).fit(xhs.flatten(start_dim=1)))\n",
    "    return nn_clfs,x_hidden\n",
    "\n",
    "\n",
    "def get_negs(model,nn_clfs,train_data,train_hidden,x,y,hl=3,\\\n",
    "            input_shape=(3,32,32),device=DEVICE):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hidden = model.feature(x.to(device)).cpu()\n",
    "    n_neighbors = nn_clfs[0].n_neighbors*9\n",
    "    y_class = [y==i for i in range(10)]\n",
    "    x_class = [x_hidden[yy] for yy in y_class]\n",
    "    nns = []\n",
    "    for i,xx in enumerate(x_class):\n",
    "        for j in range(10):\n",
    "            if j != i:\n",
    "                nn_inds = nn_clfs[j].kneighbors(xx.flatten(start_dim=1),\\\n",
    "                                                return_distance=False)\n",
    "                if (i == 0 and j == 1) or (i > 0 and j == 0):\n",
    "                    neib_col = train_data[j][torch.LongTensor(nn_inds)]\n",
    "                else:\n",
    "                    neib_col = torch.cat((neib_col,\\\n",
    "                                          train_data[j][torch.LongTensor(nn_inds)]),1)\n",
    "        nns.append(neib_col)\n",
    "    nns = torch.cat(nns,dim=0)\n",
    "    nns_reordered = torch.zeros((x.size(0),n_neighbors,)+input_shape)\n",
    "    start_ind = 0\n",
    "    for yy in y_class:\n",
    "        end_ind = start_ind+yy.sum()\n",
    "        nns_reordered[yy] = nns[start_ind:end_ind]\n",
    "        start_ind = end_ind\n",
    "    return nns_reordered.reshape((-1,)+input_shape),x_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KnnAttack(inp, y_inp, nbd, model, x_ot=None, rl=True,\\\n",
    "              eps=4/255, step=2/255, it=10, lamb = 10, DEVICE=DEVICE):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    eta = torch.FloatTensor(*inp.shape).uniform_(-eps, eps)\n",
    "    inp = inp.to(DEVICE)\n",
    "    eta = eta.to(DEVICE)\n",
    "    eta.requires_grad = True\n",
    "    inp.requires_grad = True\n",
    "    #feature = model.feature(x_adv.to(DEVICE))\n",
    "    for i in range(it):\n",
    "        inpadv = inp + eta\n",
    "\n",
    "        affinity = calc_affinity(model.feature(nbd.to(DEVICE)),\\\n",
    "                                 model.feature(inpadv.to(DEVICE))) / 9\n",
    "        \n",
    "        if relax:\n",
    "            affinity = affinity\n",
    "        else:\n",
    "            negaff = calc_affinity(model.feature(x_neg.to(DEVICE)),\\\n",
    "                            model.feature(inpadv.to(DEVICE))) / 9\n",
    "            affinity = torch.log(torch.exp(affinity) / (torch.exp(affinity)+torch.exp(negaff)))\n",
    "        affinity = - affinity\n",
    "        pred_adv = model(inpadv)[-1]\n",
    "        loss_ce = - loss_fn(pred_adv, y_inp.to(DEVICE))\n",
    "        loss = loss_ce + lamb*affinity\n",
    "        grad_sign = torch.autograd.grad(loss, inpadv, only_inputs=True,\\\n",
    "                                        retain_graph = False)[0].sign()\n",
    "        #affinity.backward()\n",
    "        pert = step * grad_sign\n",
    "        inpadv = (inpadv-pert).clamp(0.0,1.0)\n",
    "        tempeta = (inpadv - inp).clamp(-eps, eps)\n",
    "        eta = tempeta\n",
    "    return inp+eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(trainset.targets)\n",
    "train_data = [torch.FloatTensor(trainset.data[y_train==i].transpose(0,3,1,2)/255.) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/30 epochs::  75%|███████▌  | 294/391 [59:36<19:40, 12.17s/it, train_loss=-.0127, train_acc=0.11]   "
     ]
    }
   ],
   "source": [
    "BURN_IN = 0\n",
    "EPS = 0.02\n",
    "#use relaxation or not\n",
    "relax = False\n",
    "x_neg = None\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(),lr=1e-3,momentum=0.9,weight_decay=1e-4,nesterov=True)\n",
    "pgd = PGD(eps=8/255.,step=2/255.,max_iter=10)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer,step_size=50,gamma=0.1)\n",
    "EPOCHS = 30\n",
    "nn_clfs = None\n",
    "lt1 = 1#0.01  #penalty on knn loss\n",
    "lt2 = 100#100\n",
    "layers = 3\n",
    "for ep in range(EPOCHS):\n",
    "    \n",
    "    \n",
    "    #if ep>=BURN_IN and not (ep-BURN_IN)%1:\n",
    "    if ep == 0:\n",
    "        nn_clfs, train_hidden = build_nn_clfs(model,train_data,hidden_layer=layers,\\\n",
    "                                              n_neighbors=9)\n",
    "    if not relax:\n",
    "        neg_clfs, neg_hidden = build_neg_clfs(model,train_data,hidden_layer=layers,\\\n",
    "                                              n_neighbors=1)\n",
    "    \n",
    "    train_loss = 0.\n",
    "    train_correct = 0.\n",
    "    train_total = 0.\n",
    "\n",
    "    with tqdm(trainloader,desc=f\"{ep+1}/{EPOCHS} epochs:\") as t:\n",
    "        for i,(x,y) in enumerate(t):\n",
    "            model.train()\n",
    "            if nn_clfs is not None:\n",
    "                x_mem, _ = get_nns(model,nn_clfs,train_data,train_hidden,x,y)\n",
    "                if not relax:\n",
    "                    x_neg, _ = get_negs(model,neg_clfs,train_data,neg_hidden,x,y)\n",
    "                x_adv = KnnAttack(x, y, x_mem, model, x_ot = x_neg, rl = relax, eps=4/255,\\\n",
    "                                  step=2/255,it=10, lamb = lt2, DEVICE=DEVICE)\n",
    "                model.train()\n",
    "                _,out = model(x_adv.detach().to(DEVICE))\n",
    "                loss_ce = loss_fn(out,y.to(DEVICE))\n",
    "                aff = calc_affinity(model.feature(x_mem.to(DEVICE)),\\\n",
    "                                    model.feature(x_adv.to(DEVICE))) / 9\n",
    "                if relax:\n",
    "                    aff = aff\n",
    "                else:\n",
    "                    negaff = calc_affinity(model.feature(x_neg.to(DEVICE)),\\\n",
    "                                    model.feature(x_adv.to(DEVICE))) / 9\n",
    "                    aff = torch.log(torch.exp(aff) / (torch.exp(aff)+torch.exp(negaff)))\n",
    "                loss = loss_ce + lt1*aff\n",
    "                train_loss += loss.item()\n",
    "                pred = out.max(1)[1].detach().cpu()\n",
    "                train_correct += (pred==y).sum().item()\n",
    "                train_total += x.size(0)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                t.set_postfix({\n",
    "                    \"train_loss\": train_loss/train_total,\n",
    "                    \"train_acc\": train_correct/train_total\n",
    "                })\n",
    "            else:\n",
    "                model.train()\n",
    "                _,out = model(x.to(DEVICE))\n",
    "                loss = loss_fn(out,y.to(DEVICE))\n",
    "                train_loss += loss.item()*x.size(0)\n",
    "                pred = out.max(dim=1)[1].detach().cpu()\n",
    "                train_correct += (pred==y).sum().item()\n",
    "                train_total += x.size(0)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                t.set_postfix({\n",
    "                    \"train_loss\": train_loss/train_total,\n",
    "                    \"train_acc\": train_correct/train_total\n",
    "                })\n",
    "            if i == len(trainloader)-1:\n",
    "                test_correct_rob = 0\n",
    "                test_correct = 0\n",
    "                test_correct_knnrob = 0\n",
    "                test_total = 0\n",
    "                for x,y in testloader:\n",
    "                    x_adv = pgd.generate(model,x,y,device=DEVICE)\n",
    "                    #knn attack (informal)\n",
    "                    nn_clfs, train_hidden = build_nn_clfs(model,train_data,\\\n",
    "                                                          hidden_layer=layers,n_neighbors=9)\n",
    "                    \n",
    "                    if not relax:\n",
    "                        neg_clfs, neg_hidden = build_neg_clfs(model,train_data,\\\n",
    "                                                          hidden_layer=layers,n_neighbors=1)\n",
    "                    \n",
    "                    x_mem, _ = get_nns(model,nn_clfs,train_data,train_hidden,x,y)\n",
    "                    if not relax:\n",
    "                        x_neg, _ = get_negs(model,neg_clfs,train_data,neg_hidden,x,y)\n",
    "                    x_adv = KnnAttack(x, y, x_mem, model, x_ot = x_neg, rl = relax, eps=4/255,\\\n",
    "                                  step=2/255,it=10, lamb = 10000, DEVICE=DEVICE)\n",
    "#                     x_knnadv = KnnAttack(x, y, x_mem, model, eps=4/255, step=2/255,\\\n",
    "#                                          it=10, lamb = 100, DEVICE=DEVICE)\n",
    "                    ####\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(x.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                        test_correct += (pred==y.to(DEVICE)).sum().item()\n",
    "                        pred_adv = model(x_adv.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                        test_correct_rob += (pred_adv==y.to(DEVICE)).sum().item()\n",
    "                        test_total += x.size(0)\n",
    "                        #knn attack acc\n",
    "                        pred_knnadv = model(x_knnadv.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                        test_correct_knnrob += (pred_knnadv==y.to(DEVICE)).sum().item()\n",
    "                        #\n",
    "                t.set_postfix({\n",
    "                    \"train_loss\": train_loss/train_total,\n",
    "                    \"train_acc\": train_correct/train_total,\n",
    "                    \"test_acc\": test_correct/test_total,\n",
    "                    \"test_acc_rob\": test_correct_rob/test_total,\n",
    "                    \"test_acc_knnrob\": test_correct_knnrob/test_total\n",
    "                })\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
