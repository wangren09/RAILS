{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "#from models.VGG import VGG\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from pgd import PGD\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT = \"./datasets\"\n",
    "\n",
    "trainset = datasets.CIFAR10(root=ROOT,train=True,transform=transforms.ToTensor())\n",
    "trainloader = DataLoader(trainset,shuffle=True,batch_size=100)\n",
    "\n",
    "testset = datasets.CIFAR10(root=ROOT,train=False,transform=transforms.ToTensor())\n",
    "testloader = DataLoader(testset,shuffle=True,batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = VGG()\n",
    "\n",
    "# class Extractor(nn.Module):\n",
    "#     def __init__(self,model,hidden_layer):\n",
    "#         super(Extractor,self).__init__()\n",
    "#         self.hidden_layer = hidden_layer\n",
    "#         if len(self.hidden_layer) == 1:\n",
    "#             self.feature,self.out = self._get_layer(model)\n",
    "#         if len(self.hidden_layer) == 2:\n",
    "#             self.feature1, self.feature, self.out = self._get_layer(model)\n",
    "#         if len(self.hidden_layer) == 3:\n",
    "#             self.feature1, self.feature2, self.feature, self.out = self._get_layer(model)\n",
    "#     def _get_layer(self,model):\n",
    "#         children = list(model.named_children())\n",
    "#         for i,(name,mod) in enumerate(children):\n",
    "#             if \"classifier\" in name:\n",
    "#                 break\n",
    "#         children.insert(i,(\"flatten\",nn.Flatten(start_dim=1)))\n",
    "#         if len(self.hidden_layer) == 1:\n",
    "#             return nn.Sequential(OrderedDict(children[:self.hidden_layer[0]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[0]+1:]))\n",
    "#         elif len(self.hidden_layer) == 2:\n",
    "#             return nn.Sequential(OrderedDict(children[:self.hidden_layer[0]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[0]+1:self.hidden_layer[1]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[1]+1:]))\n",
    "#         else:\n",
    "#             return nn.Sequential(OrderedDict(children[:self.hidden_layer[0]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[0]+1:self.hidden_layer[1]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[1]+1:self.hidden_layer[2]+1])), \\\n",
    "#             nn.Sequential(OrderedDict(children[self.hidden_layer[2]+1:]))\n",
    "#     def forward(self,x):\n",
    "#         if len(self.hidden_layer) == 3:\n",
    "#             feature1 = self.feature1(x)\n",
    "#             feature2 = self.feature2(feature1)\n",
    "#             feature = self.feature(feature2)\n",
    "#             out = self.out(feature)\n",
    "#             return [feature1,feature2,feature,out]\n",
    "#         if len(self.hidden_layer) == 2:\n",
    "#             feature1 = self.feature1(x)\n",
    "#             feature = self.feature(feature1)\n",
    "#             out = self.out(feature)\n",
    "#             return [feature1,feature,out]\n",
    "#         else:\n",
    "#             feature = self.feature(x)\n",
    "#             out = self.out(feature)\n",
    "#             return [feature,out]\n",
    "    \n",
    "# model = Extractor(model,[5])\n",
    "# model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = VGG()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        #self.features = self._make_layers(cfg[vgg_name])\n",
    "        cfg1 = [64, 64, 'M']\n",
    "        cfg2 = [128, 128, 'M']\n",
    "        cfg3 = [256, 256, 256, 'M']\n",
    "        cfg4 = [512, 512, 512, 'M']\n",
    "        cfg5 = [512, 512, 512, 'M']\n",
    "        self.f1 = self._make_layers(cfg1, 3)\n",
    "        self.f2 = self._make_layers(cfg2, 64)\n",
    "        self.f3 = self._make_layers(cfg3, 128)\n",
    "        self.f4 = self._make_layers(cfg4, 256)\n",
    "        self.f5 = self._make_layers(cfg5, 512)\n",
    "        self.layer = nn.AvgPool2d(kernel_size=1, stride=1)\n",
    "        #self.classifier = nn.Linear(512, 10)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.f1(x)\n",
    "        out2 = self.f2(out1)\n",
    "        out3 = self.f3(out2)\n",
    "        out4 = self.f4(out3)\n",
    "        out45 = self.f5(out4)\n",
    "        out5 = self.layer(out45)\n",
    "        out = out5.view(out5.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return [out4, out45, out]\n",
    "    \n",
    "\n",
    "    def _make_layers(self, cfg, in_channels):\n",
    "        layers = []\n",
    "#         in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "model = VGG()\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load(\n",
    "#     \"./model_weights/cifar_vgg16.pt\", map_location=DEVICE\n",
    "# )['state_dict'])\n",
    "# model.eval()\n",
    "# model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_clfs(model,x_train,hidden_layer=3,n_neighbors=10,\\\n",
    "                  batch_size=1000,class_size=1000,ind=None,device=DEVICE):\n",
    "    nn_clfs = []\n",
    "    x_hidden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k,x in enumerate(x_train):\n",
    "            #x = x[np.random.choice(np.arange(x.size(0)),size=class_size,replace=False)]\n",
    "            x = x[ind[k]]\n",
    "            xhs = []\n",
    "            for i in range(0,x.size(0),batch_size):\n",
    "                xhs.append(model(x[i:i+batch_size].to(device))[1].cpu())\n",
    "            xhs = torch.cat(xhs,dim=0)\n",
    "            x_hidden.append(xhs)\n",
    "            nn_clfs.append(NearestNeighbors(n_neighbors=n_neighbors,\\\n",
    "                                            n_jobs=-1).fit(xhs.flatten(start_dim=1)))\n",
    "    return nn_clfs,x_hidden\n",
    "\n",
    "\n",
    "def get_nns(model,nn_clfs,train_data,train_hidden,x,y,hl=3,\\\n",
    "            input_shape=(3,32,32),device=DEVICE):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hidden = model(x.to(device))[1].cpu()\n",
    "    n_neighbors = nn_clfs[0].n_neighbors\n",
    "    y_class = [y==i for i in range(10)]\n",
    "    x_class = [x_hidden[yy] for yy in y_class]\n",
    "    nns = []\n",
    "    for i,xx in enumerate(x_class):\n",
    "        nn_inds = nn_clfs[i].kneighbors(xx.flatten(start_dim=1),return_distance=False)\n",
    "        nns.append(train_data[i][torch.LongTensor(nn_inds)])\n",
    "    nns = torch.cat(nns,dim=0)\n",
    "    nns_reordered = torch.zeros((x.size(0),n_neighbors,)+input_shape)\n",
    "    start_ind = 0\n",
    "    for yy in y_class:\n",
    "        end_ind = start_ind+yy.sum()\n",
    "        nns_reordered[yy] = nns[start_ind:end_ind]\n",
    "        start_ind = end_ind\n",
    "    return nns_reordered.reshape((-1,)+input_shape),x_hidden\n",
    "\n",
    "def calc_affinity(nns,x):\n",
    "    return (nns-x.repeat_interleave(nns.size(0)//x.size(0),dim=0)\\\n",
    "           ).pow(2).sum(dim=(1,2,3)).sqrt().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neg_clfs(model,x_train,hidden_layer=3,n_neighbors=1,\\\n",
    "                  batch_size=1000,class_size=1000,ind=None,device=DEVICE):\n",
    "    nn_clfs = []\n",
    "    x_hidden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k,x in enumerate(x_train):\n",
    "            #x = x[np.random.choice(np.arange(x.size(0)),size=class_size,replace=False)]\n",
    "            x = x[ind[k]]\n",
    "            xhs = []\n",
    "            for i in range(0,x.size(0),batch_size):\n",
    "                xhs.append(model(x[i:i+batch_size].to(device))[1].cpu())\n",
    "            xhs = torch.cat(xhs,dim=0)\n",
    "            x_hidden.append(xhs)\n",
    "            nn_clfs.append(NearestNeighbors(n_neighbors=n_neighbors,\\\n",
    "                                            n_jobs=-1).fit(xhs.flatten(start_dim=1)))\n",
    "    return nn_clfs,x_hidden\n",
    "\n",
    "\n",
    "def get_negs(model,nn_clfs,train_data,train_hidden,x,y,hl=3,\\\n",
    "            input_shape=(3,32,32),device=DEVICE):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hidden = model(x.to(device))[1].cpu()\n",
    "    n_neighbors = nn_clfs[0].n_neighbors*9\n",
    "    y_class = [y==i for i in range(10)]\n",
    "    x_class = [x_hidden[yy] for yy in y_class]\n",
    "    nns = []\n",
    "    for i,xx in enumerate(x_class):\n",
    "        for j in range(10):\n",
    "            if j != i:\n",
    "                nn_inds = nn_clfs[j].kneighbors(xx.flatten(start_dim=1),\\\n",
    "                                                return_distance=False)\n",
    "                if (i == 0 and j == 1) or (i > 0 and j == 0):\n",
    "                    neib_col = train_data[j][torch.LongTensor(nn_inds)]\n",
    "                else:\n",
    "                    neib_col = torch.cat((neib_col,\\\n",
    "                                          train_data[j][torch.LongTensor(nn_inds)]),1)\n",
    "        nns.append(neib_col)\n",
    "    nns = torch.cat(nns,dim=0)\n",
    "    nns_reordered = torch.zeros((x.size(0),n_neighbors,)+input_shape)\n",
    "    start_ind = 0\n",
    "    for yy in y_class:\n",
    "        end_ind = start_ind+yy.sum()\n",
    "        nns_reordered[yy] = nns[start_ind:end_ind]\n",
    "        start_ind = end_ind\n",
    "    return nns_reordered.reshape((-1,)+input_shape),x_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KnnAttack(inp, y_inp, nbd, model, x_ot=None, rl=True,\\\n",
    "              eps=4/255, step=2/255, it=10, lamb = 10, DEVICE=DEVICE):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    eta = torch.FloatTensor(*inp.shape).uniform_(-eps, eps)\n",
    "    inp = inp.to(DEVICE)\n",
    "    eta = eta.to(DEVICE)\n",
    "    eta.requires_grad = True\n",
    "    inp.requires_grad = True\n",
    "    #feature = model.feature(x_adv.to(DEVICE))\n",
    "    for i in range(it):\n",
    "        inpadv = inp + eta\n",
    "\n",
    "        affinity = calc_affinity(model(nbd.to(DEVICE))[1],\\\n",
    "                                 model(inpadv.to(DEVICE))[1])\n",
    "        \n",
    "        if rl:\n",
    "            affinity = affinity\n",
    "        else:\n",
    "            affinity = - affinity\n",
    "            negaff = - calc_affinity(model(x_ot.to(DEVICE))[1],\\\n",
    "                            model(inpadv.to(DEVICE))[1])\n",
    "            affinity = - torch.log(torch.exp(affinity) / (torch.exp(affinity)+torch.exp(negaff)))\n",
    "        affinity = - affinity\n",
    "        pred_adv = model(inpadv)[-1]\n",
    "        loss_ce = - loss_fn(pred_adv, y_inp.to(DEVICE))\n",
    "        loss = loss_ce + lamb*affinity\n",
    "        grad_sign = torch.autograd.grad(loss, inpadv, only_inputs=True,\\\n",
    "                                        retain_graph = False)[0].sign()\n",
    "        #affinity.backward()\n",
    "        pert = step * grad_sign\n",
    "        inpadv = (inpadv-pert).clamp(0.0,1.0)\n",
    "        tempeta = (inpadv - inp).clamp(-eps, eps)\n",
    "        eta = tempeta\n",
    "    return inp+eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(trainset.targets)\n",
    "train_data = [torch.FloatTensor(trainset.data[y_train==i].transpose(0,3,1,2)/255.) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dknn import DKNN\n",
    "from dknn_attack import DKNNAttack\n",
    "\n",
    "BURN_IN = 3\n",
    "EPS = 0.02\n",
    "#use relaxation or not\n",
    "relax = False\n",
    "x_neg = None\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(),lr=1e-3,momentum=0.9,weight_decay=1e-4,nesterov=True)\n",
    "pgd = PGD(eps=8/255.,step=2/255.,max_iter=10)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer,step_size=50,gamma=0.1)\n",
    "EPOCHS = 10\n",
    "nn_clfs = None\n",
    "lt1 = 0#0.01  #penalty on knn loss\n",
    "lt2 = 0#100\n",
    "layers = 4\n",
    "class_samp_size = 1000 #number of samples per class for constructing knn structure\n",
    "nn_t_class = 5\n",
    "nn_f_class = 3\n",
    "\n",
    "\n",
    "for ep in range(EPOCHS):\n",
    "\n",
    "    \n",
    "    train_loss = 0.\n",
    "    train_correct = 0.\n",
    "    train_total = 0.\n",
    "    train_clean_correct = 0.\n",
    "\n",
    "    with tqdm(trainloader,desc=f\"{ep+1}/{EPOCHS} epochs:\") as t:\n",
    "        for i,(x,y) in enumerate(t):\n",
    "            if ep >= BURN_IN:\n",
    "                lt1 = 0.1\n",
    "                lt2 = 1000\n",
    "            #model.train()\n",
    "            model.eval()\n",
    "            if i % 10 == 0 and ep >= BURN_IN:\n",
    "#                 ind_samp_tr = [np.where(y_train==i)[0][np.random.choice(\\\n",
    "#                                      np.arange(5000),size=class_samp_size,replace=False)] for i in range(10)]\n",
    "                #ind_samp_tr_flat = [x for sublist in ind_samp_tr for x in sublist]\n",
    "                \n",
    "                ind_samp_tr = [np.random.choice(\\\n",
    "                                     np.arange(5000),size=class_samp_size,replace=False) for i in range(10)]\n",
    "            \n",
    "                ind_samp_data_tr = [np.where(y_train==i)[0][ind_samp_tr[i]] for i in range(10)]\n",
    "                \n",
    "                nn_clfs, train_hidden = build_nn_clfs(model,train_data,hidden_layer=layers,\\\n",
    "                                              n_neighbors=nn_t_class,class_size=class_samp_size,ind=ind_samp_tr)\n",
    "                if not relax:\n",
    "                    neg_clfs, neg_hidden = build_neg_clfs(model,train_data,hidden_layer=layers,\\\n",
    "                                              n_neighbors=nn_f_class,class_size=class_samp_size,ind=ind_samp_tr)\n",
    "                    \n",
    "                data_samp = [torch.FloatTensor(trainset.data[ind_samp_data_tr[i]].transpose(0,3,1,2)/255.)\\\n",
    "                                   for i in range(10)]\n",
    "            \n",
    "            \n",
    "            if nn_clfs is not None:\n",
    "                x_mem, _ = get_nns(model,nn_clfs,data_samp,train_hidden,x,y)\n",
    "                if not relax:\n",
    "                    x_neg, _ = get_negs(model,neg_clfs,data_samp,neg_hidden,x,y)\n",
    "                x_adv = KnnAttack(x, y, x_mem, model, x_ot = x_neg, rl = relax, eps=4/255,\\\n",
    "                                  step=2/255,it=10, lamb = lt2, DEVICE=DEVICE)\n",
    "                \n",
    "                \n",
    "                \n",
    "                model.train()\n",
    "                *_,out = model(x_adv.detach().to(DEVICE))\n",
    "                \n",
    "                *_,out_clean = model(x.detach().to(DEVICE))\n",
    "                pred_clean = out_clean.max(1)[1].detach().cpu()\n",
    "                train_clean_correct += (pred_clean==y).sum().item()\n",
    "                \n",
    "                loss_ce = loss_fn(out,y.to(DEVICE))\n",
    "                aff = calc_affinity(model(x_mem.to(DEVICE))[1],\\\n",
    "                                    model(x_adv.to(DEVICE))[1])\n",
    "                if relax:\n",
    "                    aff = aff\n",
    "                else:\n",
    "                    aff = -aff\n",
    "                    negaff = -calc_affinity(model(x_neg.to(DEVICE))[1],\\\n",
    "                                    model(x_adv.to(DEVICE))[1])\n",
    "                    aff = -torch.log(torch.exp(aff) / (torch.exp(aff)+torch.exp(negaff)))\n",
    "                loss = loss_ce + lt1*aff\n",
    "                train_loss += loss.item()\n",
    "                pred = out.max(1)[1].detach().cpu()\n",
    "                train_correct += (pred==y).sum().item()\n",
    "                train_total += x.size(0)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                t.set_postfix({\n",
    "                    \"train_loss\": train_loss/train_total,\n",
    "                    \"train_clean_acc\": train_clean_correct/train_total,\n",
    "                    \"train_acc\": train_correct/train_total\n",
    "                })\n",
    "            else:\n",
    "                model.train()\n",
    "                *_,out = model(x.to(DEVICE))\n",
    "                loss = loss_fn(out,y.to(DEVICE))\n",
    "                train_loss += loss.item()*x.size(0)\n",
    "                pred = out.max(dim=1)[1].detach().cpu()\n",
    "                train_correct += (pred==y).sum().item()\n",
    "                train_total += x.size(0)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                t.set_postfix({\n",
    "                    \"train_loss\": train_loss/train_total,\n",
    "                    \"train_acc\": train_correct/train_total\n",
    "                })\n",
    "            if i == len(trainloader)-1 and ep >=BURN_IN:\n",
    "                count_up_knn = 0\n",
    "                test_correct_rob = 0\n",
    "                test_correct = 0\n",
    "                test_correct_knn = 0\n",
    "                test_correct_knnrob = 0\n",
    "                test_correct_knnpgd = 0\n",
    "                test_total = 0\n",
    "                test_total_knn = 0\n",
    "                count_knn_eval = 0\n",
    "                \n",
    "#                 index = np.random.choice(np.arange(50000),\\\n",
    "#                          size=1000,replace=False)\n",
    "                \n",
    "\n",
    "#                 ind_samp =  [np.where(y_train==i)[0][np.random.choice(\\\n",
    "#                                      np.arange(5000),size=class_samp_size,replace=False)] for i in range(10)]\n",
    "        \n",
    "                ind_samp =  [np.random.choice(\\\n",
    "                                     np.arange(5000),size=class_samp_size,replace=False) for i in range(10)]\n",
    "                ind_samp_data = [np.where(y_train==i)[0][ind_samp[i]] for i in range(10)]\n",
    "                ind_samp_flat = [x for sublist in ind_samp_data for x in sublist]\n",
    "                train_samp = torch.FloatTensor(trainset.data.\\\n",
    "                                               transpose(0,3,1,2)/255.)[ind_samp_flat]\n",
    "                y_samp = torch.LongTensor(y_train)[ind_samp_flat]\n",
    "                \n",
    "#                 train_samp = torch.FloatTensor(trainset.data.\\\n",
    "#                                                transpose(0,3,1,2)/255.)[index]\n",
    "#                 y_samp = torch.LongTensor(y_train)[index]\n",
    "                model.eval()\n",
    "                dknn = DKNN(model, train_samp, y_samp,\\\n",
    "                            hidden_layers=[4], device=DEVICE)\n",
    "                    \n",
    "#                 dknnatt = DKNNAttack(\n",
    "#                     model,\n",
    "#                     train_samp,\n",
    "#                     y_samp,\n",
    "#                     hidden_layers=[4],\n",
    "#                 device=DEVICE)\n",
    "\n",
    "\n",
    "                train_data_samp = [torch.FloatTensor(trainset.data[ind_samp_data[i]].transpose(0,3,1,2)/255.)\\\n",
    "                                   for i in range(10)]\n",
    "                nn_clfs, train_hidden = build_nn_clfs(model,train_data,hidden_layer=layers,\\\n",
    "                                                n_neighbors=nn_t_class,class_size=class_samp_size,ind=ind_samp)\n",
    "\n",
    "                if not relax:\n",
    "                    neg_clfs, neg_hidden = build_neg_clfs(model,train_data,hidden_layer=layers,\\\n",
    "                                                    n_neighbors=nn_f_class,class_size=class_samp_size,ind=ind_samp)\n",
    "\n",
    "                    \n",
    "#                 data_samp = [torch.FloatTensor(trainset.data[ind_samp_tr[i]].transpose(0,3,1,2)/255.)\\\n",
    "#                                    for i in range(10)]\n",
    "\n",
    "                for x,y in testloader:\n",
    "                    count_knn_eval = count_knn_eval + 1\n",
    "                    x_adv = pgd.generate(model,x,y,device=DEVICE)\n",
    "                    #knn attack (informal)\n",
    "                    if count_knn_eval < count_up_knn:\n",
    "                        x_mem, _ = get_nns(model,nn_clfs,train_data_samp,train_hidden,x,y)\n",
    "                        if not relax:\n",
    "                            x_neg, _ = get_negs(model,neg_clfs,train_data_samp,neg_hidden,x,y)\n",
    "                        x_knnadv = KnnAttack(x, y, x_mem, model, x_ot = x_neg, rl = relax, eps=8/255,\\\n",
    "                                      step=2/255,it=10, lamb = 1000, DEVICE=DEVICE)\n",
    "                    \n",
    "\n",
    "            \n",
    "            \n",
    "#                     x_knnadv = dknnatt.generate(x, y)\n",
    "    \n",
    "    \n",
    "    ####\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(x.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                        test_correct += (pred==y.to(DEVICE)).sum().item()\n",
    "                        pred_adv = model(x_adv.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                        test_correct_rob += (pred_adv==y.to(DEVICE)).sum().item()\n",
    "                        test_total += x.size(0)\n",
    "                        \n",
    "                        if count_knn_eval < count_up_knn:\n",
    "                        #knn attack acc\n",
    "                            pred_dknn = dknn(x.to(DEVICE)).argmax(axis=1)\n",
    "                            pred_knnadv = dknn(x_knnadv.to(DEVICE)).argmax(axis=1)\n",
    "                            pred_knnpgdadv = dknn(x_adv.to(DEVICE)).argmax(axis=1)\n",
    "#                         #pred_knnadv = model(x_knnadv.to(DEVICE))[-1].max(dim=1)[1]\n",
    "                            test_correct_knn += (pred_dknn==y.numpy()).astype(\"float\").sum()\n",
    "                            test_correct_knnrob += (pred_knnadv==y.numpy()).astype(\"float\").sum()\n",
    "                            test_correct_knnpgd += (pred_knnpgdadv==y.numpy()).astype(\"float\").sum()\n",
    "                            test_total_knn += x.size(0)\n",
    "                        #\n",
    "                t.set_postfix({\n",
    "                    \"train_loss\": train_loss/train_total,\n",
    "                    \"train_acc\": train_correct/train_total,\n",
    "                    \"test_acc\": test_correct/test_total,\n",
    "                    \"test_acc_rob\": test_correct_rob/test_total,\n",
    "                    \"test_acc_knn\": test_correct_knn/test_total_knn,\n",
    "                    \"test_acc_knnrob\": test_correct_knnrob/test_total_knn,\n",
    "                    \"test_acc_knnpgd\": test_correct_knnpgd/test_total_knn\n",
    "                })\n",
    "                state = {\n",
    "                        'state_dict': model.state_dict()\n",
    "                    }\n",
    "                torch.save(state, 'models/relaxtest0d1and1000.pt')\n",
    "                print('saved')\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(loss_ce)\n",
    "# print(aff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
