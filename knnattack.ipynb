{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (f1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (f5): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "#from models.VGG import VGG\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from pgd import PGD\n",
    "from tqdm import tqdm\n",
    "from dknn import DKNN\n",
    "\n",
    "ROOT = \"./datasets\"\n",
    "\n",
    "trainset = datasets.CIFAR10(root=ROOT,train=True,transform=transforms.ToTensor())\n",
    "trainloader = DataLoader(trainset,shuffle=True,batch_size=100)\n",
    "\n",
    "testset = datasets.CIFAR10(root=ROOT,train=False,transform=transforms.ToTensor())\n",
    "testloader = DataLoader(testset,shuffle=True,batch_size=50)\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        #self.features = self._make_layers(cfg[vgg_name])\n",
    "        cfg1 = [64, 64, 'M']\n",
    "        cfg2 = [128, 128, 'M']\n",
    "        cfg3 = [256, 256, 256, 'M']\n",
    "        cfg4 = [512, 512, 512, 'M']\n",
    "        cfg5 = [512, 512, 512, 'M']\n",
    "        self.f1 = self._make_layers(cfg1, 3)\n",
    "        self.f2 = self._make_layers(cfg2, 64)\n",
    "        self.f3 = self._make_layers(cfg3, 128)\n",
    "        self.f4 = self._make_layers(cfg4, 256)\n",
    "        self.f5 = self._make_layers(cfg5, 512)\n",
    "        self.layer = nn.AvgPool2d(kernel_size=1, stride=1)\n",
    "        #self.classifier = nn.Linear(512, 10)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.f1(x)\n",
    "        out2 = self.f2(out1)\n",
    "        out3 = self.f3(out2)\n",
    "        out4 = self.f4(out3)\n",
    "        out45 = self.f5(out4)\n",
    "        out5 = self.layer(out45)\n",
    "        out = out5.view(out5.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return [out3, out4, out45, out]\n",
    "    \n",
    "\n",
    "    def _make_layers(self, cfg, in_channels):\n",
    "        layers = []\n",
    "#         in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "model = VGG()\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    \"./model_weights/cifar_vgg16.pt\", map_location=DEVICE\n",
    ")['state_dict'])\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_clfs(model,x_train,hidden_layer=1,n_neighbors=10,\\\n",
    "                  batch_size=1000,class_size=1000,ind=None,device=DEVICE):\n",
    "    nn_clfs = []\n",
    "    x_hidden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k,x in enumerate(x_train):\n",
    "            #x = x[np.random.choice(np.arange(x.size(0)),size=class_size,replace=False)]\n",
    "            x = x[ind[k]]\n",
    "            xhs = []\n",
    "            for i in range(0,x.size(0),batch_size):\n",
    "                xhs.append(model(x[i:i+batch_size].to(device))[hidden_layer].cpu())\n",
    "            xhs = torch.cat(xhs,dim=0)\n",
    "            x_hidden.append(xhs)\n",
    "            nn_clfs.append(NearestNeighbors(n_neighbors=n_neighbors,\\\n",
    "                                            n_jobs=-1).fit(xhs.flatten(start_dim=1)))\n",
    "    return nn_clfs,x_hidden\n",
    "\n",
    "\n",
    "def get_nns(model,nn_clfs,train_data,train_hidden,x,y,hl=1,\\\n",
    "            input_shape=(3,32,32),device=DEVICE):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hidden = model(x.to(device))[hl].cpu()\n",
    "    n_neighbors = nn_clfs[0].n_neighbors\n",
    "    y_class = [y==i for i in range(10)]\n",
    "    x_class = [x_hidden[yy] for yy in y_class]\n",
    "    nns = []\n",
    "    for i,xx in enumerate(x_class):\n",
    "        nn_inds = nn_clfs[i].kneighbors(xx.flatten(start_dim=1),return_distance=False)\n",
    "        nns.append(train_data[i][torch.LongTensor(nn_inds)])\n",
    "    nns = torch.cat(nns,dim=0)\n",
    "    nns_reordered = torch.zeros((x.size(0),n_neighbors,)+input_shape)\n",
    "    start_ind = 0\n",
    "    for yy in y_class:\n",
    "        end_ind = start_ind+yy.sum()\n",
    "        nns_reordered[yy] = nns[start_ind:end_ind]\n",
    "        start_ind = end_ind\n",
    "    return nns_reordered.reshape((-1,)+input_shape),x_hidden\n",
    "\n",
    "\n",
    "\n",
    "def build_neg_clfs(model,x_train,hidden_layer=1,n_neighbors=1,\\\n",
    "                  batch_size=1000,class_size=1000,ind=None,device=DEVICE):\n",
    "    nn_clfs = []\n",
    "    x_hidden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k,x in enumerate(x_train):\n",
    "            #x = x[np.random.choice(np.arange(x.size(0)),size=class_size,replace=False)]\n",
    "            x = x[ind[k]]\n",
    "            xhs = []\n",
    "            for i in range(0,x.size(0),batch_size):\n",
    "                xhs.append(model(x[i:i+batch_size].to(device))[hidden_layer].cpu())\n",
    "            xhs = torch.cat(xhs,dim=0)\n",
    "            x_hidden.append(xhs)\n",
    "            nn_clfs.append(NearestNeighbors(n_neighbors=n_neighbors,\\\n",
    "                                            n_jobs=-1).fit(xhs.flatten(start_dim=1)))\n",
    "    return nn_clfs,x_hidden\n",
    "\n",
    "\n",
    "def get_negs(model,nn_clfs,train_data,train_hidden,x,y,hl=1,\\\n",
    "            input_shape=(3,32,32),device=DEVICE):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hidden = model(x.to(device))[hl].cpu()\n",
    "    n_neighbors = nn_clfs[0].n_neighbors*9\n",
    "    y_class = [y==i for i in range(10)]\n",
    "    x_class = [x_hidden[yy] for yy in y_class]\n",
    "    nns = []\n",
    "    for i,xx in enumerate(x_class):\n",
    "        for j in range(10):\n",
    "            if j != i:\n",
    "                nn_inds = nn_clfs[j].kneighbors(xx.flatten(start_dim=1),\\\n",
    "                                                return_distance=False)\n",
    "                if (i == 0 and j == 1) or (i > 0 and j == 0):\n",
    "                    neib_col = train_data[j][torch.LongTensor(nn_inds)]\n",
    "                else:\n",
    "                    neib_col = torch.cat((neib_col,\\\n",
    "                                          train_data[j][torch.LongTensor(nn_inds)]),1)\n",
    "        nns.append(neib_col)\n",
    "    nns = torch.cat(nns,dim=0)\n",
    "    nns_reordered = torch.zeros((x.size(0),n_neighbors,)+input_shape)\n",
    "    start_ind = 0\n",
    "    for yy in y_class:\n",
    "        end_ind = start_ind+yy.sum()\n",
    "        nns_reordered[yy] = nns[start_ind:end_ind]\n",
    "        start_ind = end_ind\n",
    "    return nns_reordered.reshape((-1,)+input_shape),x_hidden\n",
    "\n",
    "\n",
    "def calc_affinity(nns,x):\n",
    "    #nns - (batchsize, nearest neighbor size, ch1, ch2, ch3)\n",
    "    s1, s2, s3, s4, s5 = nns.size()\n",
    "    x = x.repeat_interleave(s2,dim=0).reshape((s1, s2, s3, s4, s5))\n",
    "    return (nns-x).pow(2).sum(dim=(1,2,3,4)).sqrt() / s2#.mean()\n",
    "\n",
    "def KnnAttack(inp, y_inp, nbd, model, x_ot=None, rl=True,\\\n",
    "              eps=8/255, step=2/255, it=10, lamb = 10, layer=1, ch=False, DEVICE=DEVICE):\n",
    "    choice = ch #set to False if using the softmax-format version\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    eta = torch.FloatTensor(*inp.shape).uniform_(-eps, eps)\n",
    "    inp = inp.to(DEVICE)\n",
    "    eta = eta.to(DEVICE)\n",
    "    eta.requires_grad = True\n",
    "    inp.requires_grad = True\n",
    "    tau = 1\n",
    "    shape = model(inp[:2].to(DEVICE))[layer].size()\n",
    "    s1 = shape[1]\n",
    "    s2 = shape[2]\n",
    "    s3 = shape[3]\n",
    "    d1 = inp.size(0)\n",
    "    d2 = nbd.size(0) // d1\n",
    "    neibor_sc_rep = model(nbd.to(DEVICE))[layer].reshape((d1,d2,s1,s2,s3))\n",
    "    if not rl:\n",
    "        d2_cl = x_ot.size(0) // d1\n",
    "        neibor_dc_rep = model(x_ot.to(DEVICE))[layer].reshape((d1,d2_cl,s1,s2,s3))\n",
    "        #only used for choice False of the untargeted attack\n",
    "        if not choice:\n",
    "            size_cl = d2_cl // 9\n",
    "                #ind.append([k for i in range(d1) for k in range(j*size_cl+d2_cl*i,(j+1)*size_cl+d2_cl*i)])\n",
    "        ###\n",
    "        \n",
    "    \n",
    "    \n",
    "    for i in range(it):\n",
    "        inpadv = inp + eta\n",
    "\n",
    "        affinity = calc_affinity(neibor_sc_rep,model(inpadv.to(DEVICE))[layer])\n",
    "        \n",
    "        if rl:\n",
    "            affinity = affinity\n",
    "        else:\n",
    "            affinity = - affinity\n",
    "            if choice:\n",
    "                negaff = - calc_affinity(neibor_dc_rep,model(inpadv.to(DEVICE))[layer])\n",
    "                affinity = - torch.log(torch.exp(affinity / tau) /\\\n",
    "                                       (torch.exp(affinity / tau)+torch.exp(negaff / tau)))\n",
    "            #only used for choice 1 of the untargeted attack\n",
    "            else:\n",
    "                negaff = 0\n",
    "                for j in range(9):\n",
    "                    tempaff = - calc_affinity(neibor_dc_rep[:,j*size_cl:(j+1)*size_cl],\\\n",
    "                                              model(inpadv.to(DEVICE))[layer])\n",
    "                    negaff += torch.exp(tempaff / tau)\n",
    "                affinity = - torch.log(torch.exp(affinity / tau) /\\\n",
    "                                   (torch.exp(affinity / tau)+negaff))\n",
    "            #######\n",
    "            \n",
    "        affinity = - affinity.mean()\n",
    "        pred_adv = model(inpadv)[-1]\n",
    "        loss_ce = - loss_fn(pred_adv, y_inp.to(DEVICE))\n",
    "        loss = 0*loss_ce + lamb*affinity\n",
    "        grad_sign = torch.autograd.grad(loss, inpadv, only_inputs=True,\\\n",
    "                                        retain_graph = False)[0].sign()\n",
    "        #affinity.backward()\n",
    "        pert = step * grad_sign\n",
    "        inpadv = (inpadv-pert).clamp(0.0,1.0)\n",
    "        tempeta = (inpadv - inp).clamp(-eps, eps)\n",
    "        eta = tempeta\n",
    "    return inp+eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_acc': 0.9133333333333333, 'test_acc_rob': 0.3111111111111111, 'test_acc_knn': 0.68, 'test_acc_knnrob': 0.35777777777777775, 'test_acc_knnpgd': 0.5422222222222223}\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(trainset.targets)\n",
    "train_data = [torch.FloatTensor(trainset.data[y_train==i].transpose(0,3,1,2)/255.) for i in range(10)]\n",
    "\n",
    "\n",
    "#use relaxation or not; In the attack, we will mainly consider relax=False\n",
    "relax = False\n",
    "x_neg = None\n",
    "\n",
    "#use which version of contrastive loss, set to False if using the softmax-format version\n",
    "choice = False\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(),lr=1e-3,momentum=0.9,weight_decay=1e-4,nesterov=True)\n",
    "pgd = PGD(eps=8/255.,step=2/255.,max_iter=10)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer,step_size=50,gamma=0.1)\n",
    "nn_clfs = None\n",
    "lt1 = 0#0.01  #penalty on knn loss\n",
    "lt2 = 0#100\n",
    "layers = 0 #the selected layers, choices are 0, 1, 2, 3; 0/1/2 is the third/fourth/fifth layer, 3 is the output\n",
    "class_samp_size = 1000 #number of samples per class for constructing knn structure\n",
    "nn_t_class = 9 #k for the true class\n",
    "nn_f_class = 9 #k for the other classes\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "count_up_knn = 10\n",
    "test_correct_rob = 0\n",
    "test_correct = 0\n",
    "test_correct_knn = 0\n",
    "test_correct_knnrob = 0\n",
    "test_correct_knnpgd = 0\n",
    "test_total = 0\n",
    "test_total_knn = 0\n",
    "count_knn_eval = 0\n",
    "\n",
    "\n",
    "ind_samp =  [np.random.choice(\\\n",
    "                     np.arange(5000),size=class_samp_size,replace=False) for i in range(10)]\n",
    "ind_samp_data = [np.where(y_train==i)[0][ind_samp[i]] for i in range(10)]\n",
    "ind_samp_flat = [x for sublist in ind_samp_data for x in sublist]\n",
    "train_samp = torch.FloatTensor(trainset.data.\\\n",
    "                               transpose(0,3,1,2)/255.)[ind_samp_flat]\n",
    "y_samp = torch.LongTensor(y_train)[ind_samp_flat]\n",
    "\n",
    "\n",
    "model.eval()\n",
    "dknn = DKNN(model, train_samp, y_samp,\\\n",
    "            hidden_layers=[layers+2], device=DEVICE)\n",
    "\n",
    "\n",
    "train_data_samp = [torch.FloatTensor(trainset.data[ind_samp_data[i]].transpose(0,3,1,2)/255.)\\\n",
    "                   for i in range(10)]\n",
    "nn_clfs, train_hidden = build_nn_clfs(model,train_data,hidden_layer=layers,\\\n",
    "                                n_neighbors=nn_t_class,class_size=class_samp_size,ind=ind_samp)\n",
    "\n",
    "if not relax:\n",
    "    neg_clfs, neg_hidden = build_neg_clfs(model,train_data,hidden_layer=layers,\\\n",
    "                                    n_neighbors=nn_f_class,class_size=class_samp_size,ind=ind_samp)\n",
    "\n",
    "\n",
    "for x,y in testloader:\n",
    "    count_knn_eval = count_knn_eval + 1\n",
    "    \n",
    "    if count_knn_eval < count_up_knn:\n",
    "        x_adv = pgd.generate(model,x,y,device=DEVICE)\n",
    "        x_mem, _ = get_nns(model,nn_clfs,train_data_samp,train_hidden,x,y,hl=layers)\n",
    "        if not relax:\n",
    "            x_neg, _ = get_negs(model,neg_clfs,train_data_samp,neg_hidden,x,y,hl=layers)\n",
    "        x_knnadv = KnnAttack(x, y, x_mem, model, x_ot = x_neg, rl = relax, eps=8/255,\\\n",
    "                      step=2/255,it=20, lamb = 1000, layer=layers, ch=choice, DEVICE=DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if count_knn_eval < count_up_knn:\n",
    "            pred = model(x.to(DEVICE))[-1].max(dim=1)[1]\n",
    "            test_correct += (pred==y.to(DEVICE)).sum().item()\n",
    "            pred_adv = model(x_adv.to(DEVICE))[-1].max(dim=1)[1]\n",
    "            test_correct_rob += (pred_adv==y.to(DEVICE)).sum().item()\n",
    "            test_total += x.size(0)\n",
    "\n",
    "        \n",
    "        #knn attack acc\n",
    "            pred_dknn = dknn(x.to(DEVICE)).argmax(axis=1)\n",
    "            pred_knnadv = dknn(x_knnadv.to(DEVICE)).argmax(axis=1)\n",
    "            pred_knnpgdadv = dknn(x_adv.to(DEVICE)).argmax(axis=1)\n",
    "#                         #pred_knnadv = model(x_knnadv.to(DEVICE))[-1].max(dim=1)[1]\n",
    "            test_correct_knn += (pred_dknn==y.numpy()).astype(\"float\").sum()\n",
    "            test_correct_knnrob += (pred_knnadv==y.numpy()).astype(\"float\").sum()\n",
    "            test_correct_knnpgd += (pred_knnpgdadv==y.numpy()).astype(\"float\").sum()\n",
    "            test_total_knn += x.size(0)\n",
    "        #\n",
    "print({\n",
    "    \"test_acc\": test_correct/test_total,\n",
    "    \"test_acc_rob\": test_correct_rob/test_total,\n",
    "    \"test_acc_knn\": test_correct_knn/test_total_knn,\n",
    "    \"test_acc_knnrob\": test_correct_knnrob/test_total_knn,\n",
    "    \"test_acc_knnpgd\": test_correct_knnpgd/test_total_knn\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
